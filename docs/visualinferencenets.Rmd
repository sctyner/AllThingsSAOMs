# Model Diagnostics for Stochastic Actor-Oriented Models Using Visual Inference {#ch2}

**Abstract**: In this chapter, I will use the visual inference lineup protocol to determine which aspects of SAOMs are visually discoverable. First, I explore the relationship between the addition of statistically significant parameters to a SAOM and the visual side effects of the inclusion of the additional significant parameter. Using visual inference, I will determine whether the addition of the significant parameter changes the observable structure of networks belonging to the model. Then, I will simulate networks from SAOMs using the multitude of possible parameter values available and I will use visual inference protocol to determine which parameters, when included in the model, make significant changes to the appearance of the networks. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE)
library(dplyr)
library(ggplot2)
library(geomnet)
library(tidyr)
library(RSiena)
library(knitr)
```

## Introduction

The introduction of this chapter will start with a shortened introduction to the rate function and objective function of a SAOM. I do not include it here for now because these functions are introduced in great detail in Section \@ref(saoms). 

The R package `RSiena` provides methods for estimation of stochastic actor-oriented models for social network analysis [@Rsoft; @RSiena]. In this software, there are many possible parameter values that can be included into a model, but no ways of performing model selection. There are, however, some simple $t$-type and Wald-type tests of parameters that can be used to determine the significance of a parameter included in the objective function. This is crucial to the statistical analysis of the network in order to better understand the mechanism underlying the data. When studying a social network, it is also pertinent to the analysis to visualize the data. There is not, however, a way to know just which parameter values are affecting the network structure seen in the visualization. In this paper, we explore the visible effects of different model parameters, and use the lineup protocal of @Bujaetal to perform visual tests of model parameters. 

## Data {#smallfriends}

The data we use in our first experiment is a small subset of 16 actors in the dynamic friendship network collected by @friendsdata. This data is made available on the [RSiena webpage](http://www.stats.ox.ac.uk/~snijders/siena/Glasgow_data.htm). It is a study of teenage girls and the changes in their friendship network overtime, and it also includes covariate information on the girls, such as their drinking and smoking behavior or whether or not they participate in school sports. We chose to subset the data to this small network to decrease the cognitive load on our experiment's subjects. The subset contained three waves of data, 16 girls and the relationships between them, and the drinking behavior of each actor at each of the three waves. This specific subset was chosen because it showed somewhat higher connectivity than other subsets, as determined by the adjacency matrix visualization of each wave shown in Figure \@ref(fig:smallfriends). The first wave of our small network, which is conditioned on in estimation, is shown in Figure \@ref(fig:wave1). The covariate, drinking behavior, is coded in the original data set as an integer from 1-5: 1 means no consumption of alchohol, 2 is consumption once or twice a year, 3 is once a month, 4 is once a week, and 5 alcohol consupmtion more than once a week.

```{r smallfriends, echo=FALSE, fig.width=8, fig.height=3, message = FALSE, warning = FALSE, fig.cap="Adjacency matrices describing friendship relations between 50 students in waves 1,2, and 3 of the friendship study of \\@friendsdata. The red squares identify the subset we focus on for our experiment.", out.width="80%"}
#source("VisInfDocs/Code/00e_small_friends.R")
friend.data.w1 <- as.matrix(read.table("../VisInfDocs/Data/s50_data/s50-network1.dat"))
friend.data.w2 <- as.matrix(read.table("../VisInfDocs/Data/s50_data/s50-network2.dat"))
friend.data.w3 <- as.matrix(read.table("../VisInfDocs/Data/s50_data/s50-network3.dat"))

val1 <- data.frame(friend.data.w1) %>% gather(x,y, V1:V50) %>% select(y)
val2 <- data.frame(friend.data.w2) %>% gather(x,y, V1:V50) %>% select(y)
val3 <- data.frame(friend.data.w3) %>% gather(x,y, V1:V50) %>% select(y)

all_girls <- expand.grid(x = 1:50, y= 1:50)

view_rs <- data.frame(all_girls, W1 = val1, W2 = val2, W3 = val3)
names(view_rs)[3:5] <- c('w1', 'w2','w3')

view_rs %>% gather(wave, value, w1:w3) -> view_rs2
view_rs2$wave <- factor(view_rs2$wave)
levels(view_rs2$wave) <- c("Wave 1", "Wave 2", "Wave 3")
ggplot(data= view_rs2, aes(x = x, y=y)) + 
  geom_tile(aes(fill = as.factor(value))) + 
  scale_fill_manual("X likes Y", labels=c("no", "yes"), 
                    values = c('white', 'black')) +
  geom_rect(data= NULL, inherit.aes = FALSE, color = 'red',
            aes(xmin = 20, xmax = 35, ymin = 20, ymax = 35, fill =NA)) + 
  facet_wrap(~wave) + theme_bw() +
  theme(aspect.ratio=1, axis.text = element_blank(), axis.ticks = element_blank()) 

fd2.w1 <- friend.data.w1[20:35,20:35]
fd2.w2 <- friend.data.w2[20:35,20:35]
fd2.w3 <- friend.data.w3[20:35,20:35]
# read in covariate data
drink <- as.matrix(read.table("../VisInfDocs/Data/s50_data/s50-alcohol.dat"))
drink2 <- drink[20:35,]
```

```{r wave1, echo=FALSE, message=FALSE, warning=FALSE, fig.height=5, fig.cap="Network of friendships of wave 1 of the subset of students that we will be exploring.", fig.align='center', out.width='50%'}
library(sna)
library(network)
actual1 <- merge(data.frame(as.edgelist(as.network(fd2.w1))), 
                 data.frame(id = 1:16, drink = drink2[,1]), 
                 by.x = "X1", by.y = "id", all = T)
for (j in 1:nrow(actual1)){
      if (!(actual1$X1[j] %in% actual1$X2) & is.na(actual1$X2[j])){
        actual1$X2[j] <- actual1$X1[j]
      } else {actual1$X2[j] <- actual1$X2[j]}
}
actual2 <- merge(data.frame(as.edgelist(as.network(fd2.w2))), 
                 data.frame(id = 1:16, drink = drink2[,2]), 
                 by.x = "X1", by.y = "id", all = T)
for (j in 1:nrow(actual2)){
      if (!(actual2$X1[j] %in% actual2$X2) & is.na(actual2$X2[j])){
        actual2$X2[j] <- actual2$X1[j]
      } else {actual2$X2[j] <- actual2$X2[j]}
    }
actual1$wave <- 1
actual2$wave <- 2

waves <- rbind(actual1, actual2)
library(geomnet)
actual1$behaviour <- factor(actual1$drink)
levels(actual1$behaviour) <- c("None", "Once or twice a year", "Once a month", "Once a week")
ggplot(data = actual1, aes(from_id = X1, to_id = X2)) + 
  geom_net(label = TRUE, hjust = 0.5, vjust=0.5, size=10,  
           fiteach = T, labelcolour = "grey20", directed = T,
           arrowgap = .03, aes(colour = behaviour)) + 
  scale_colour_brewer("Drinking behavior", palette="YlOrRd") +
  theme_net() 

#ggplot(data = waves, aes(from_id = X1, to_id = X2)) + 
#  geom_net(label = TRUE, hjust = -.5, labelcolour = 'black', fiteach = T,
#           aes(color = as.factor(drink))) + 
#  theme_net() + theme(panel.background = element_rect(fill = "white", color = 'black')) + facet_wrap(~wave)
```

```{r modelfit, echo=FALSE, eval = FALSE, results = 'hide', message=FALSE, warning = FALSE, cache = T}
source("../VisInfDocs/Code/03c_complete_lineup_creation.R")
lineup1 <- create_smfriend_lu(null_eff_struct = null_model_eff2, test_eff_struct = eff_models_smallFriends[[39]], M = 3)
```

## Models {#m1m2}

We fit three models to the data. The first model, which we write as $M1$, is the "null model" because it includes only the abolute minimum number of parameters that should be included in the objective function SAOM: the rates of change between timepoint, the outdegree effect, and the reciprocity effect [see @snijders01 p. 371]. We also fit two "alternative models", which we write as $M_2$ and $M_3$. The alternative model $M_2$ includes one more parameter than the null model, the "jumping transitive triplet"" (JTT), which was determined to be significant by the $t$-type test in the `RSienaTest` package with a $p$-value of 0.00282. The parameters in the objective function are tested for significance using $t$-tests where test statistic is the ratio of the parameter estimate to its standard error. This parameter incorporates the actor covariate, drinking behavior, into the model. The other alternative model, $M_3$, includes one additional structural parameter, "number of doubly achieved distances two effect", whose significance was determined in the same way, with a $p$-value less than 0.0001. Finally, each of the three models includes rate parameters, $\alpha_1$ and $\alpha_2$, which represent how many opportunities for change each actor gets, on average, when moving from wave 1 to 2 and from wave 2 to 3, respectively. 

The objective function for each actor in each model is given below. 
  \begin{align*}
  f^{M_1}_{i}(\boldsymbol{\beta}, x) & = \beta_1 s_{i1}(x) +  \beta_2 s_{i2}(x) \\
  f^{M_2}_{i}(\boldsymbol{\beta}, x) & = \beta_1 s_{i1}(x) +  \beta_2 s_{i2}(x) + \beta_3 s_{i3}(x) \\
  f^{M_3}_{i}(\boldsymbol{\beta}, x) & = \beta_1 s_{i1}(x) +  \beta_2 s_{i2}(x) + \beta_4 s_{i4}(x) 
  \end{align*}

The form of the statistics, $s_{ik}(x)$ for $k = 1, \dots, 4$ are given in Table \@ref(tab:models). The outdegree parameter, $\beta_1$, represents how likely an actor is to change outgoing ties. If the estimate, $\hat{\beta}_1$, is positive, the actor is more likely to create outgoing ties, while a negative estimate leads the actor to deleting outgoing ties. This effect is highly correlated with the reciprocity parameter, $\beta_2$. A negative estimate of this parameter implies that the actor is discouraged from reciprocating its incoming ties, while a positive estimate implies that the actor is encouraged to reciprocate all ties. The additional parameter in $M_2$, $\beta_3$, is a the covariate parameter that takes into account the girls' drinking behavior. This jumping transitive triplet effect impacts the transitive closure of actors from different groups. Thus, a positive estimate encourages transitive closure when one of three actors is in a different covariate group than the other two, while a negative estimate discourages closure when one member is from a different group. An example of this type of closure is given in Figure \@ref(fig:jtt). With the directed edges we also distinguish between 'i likes j' and 'j likes i'. Finally, the doubly achieved distances effect is defined by the number of actors to whom actor $i$ is not directly tied, but to which it is connected through two different paths via at least two intermediary actors. This is a structural effect, like the density and reciprocity effects. A positive coefficient value encourages indirect ties, while a negative value discourages the formation of indirect ties in favor of direct ties. 

```{r jtt, echo=FALSE, fig.width=2, fig.height=2, message = FALSE, warning = FALSE, fig.align='center', fig.cap="Structural network effects. On the left, a jumping transitive triplet (JTT). On the right, a doubly achieved distance between $i$ and $k$. At left, a realization of a jumping transitive triplet, where $i$ is the focal actor, $j$ is the target actor, and $h$ is the intermediary. The group of the actors is represented by the shape of the node. At right, doubly achieved distance between actors $i$ and $k$.", out.width = "50%", fig.show='hold'}
jTTe <- data.frame(from = c('i', 'i', 'h'), to = c('h', 'j', 'j'))
jTTn <- data.frame(id = letters[8:10], group = c(1,1,2))

jTT <- merge(jTTe, jTTn, by.x = 'from', by.y = "id", all = T)

set.seed(12345) 
ggplot(data = jTT, aes(from_id = from, to_id = to)) + 
  geom_net(aes(shape = as.factor(group)), directed = T, label = T, 
           labelcolour='grey80',vjust = 0.5, hjust =0.5, arrowgap = .15, 
           colour = 'black', size=10, 
           ecolour = c("red", "grey40", "grey40", "grey40")) + 
  expand_limits(x=c(-0.1,1.1), y=c(-0.1,1.1)) +
  theme_net() +
  theme(legend.position = "none")

dade <- data.frame(from = c('i', 'i', 'h', 'j'), to = c('h', 'j', 'k', 'k'))
dadn <- data.frame(id = letters[8:11], group = c(1,1,1,1))

dad <- merge(dade, dadn, by.x = 'from', by.y = "id", all = T)

set.seed(12345) 
ggplot(data = dad, aes(from_id = from, to_id = to)) + 
  geom_net(aes(shape = as.factor(group)), directed = T, label = T, labelcolour='grey80',vjust = 0.5, hjust =0.5, arrowgap = .15, colour = 'black', size=10) + 
  expand_limits(x=c(-0.1,1.1), y=c(-0.1,1.1)) +
  theme_net() +
  theme(legend.position = "none")
```

### Model Fitting and Simulation {#fitting}
```{r readsimu, echo=FALSE}
 nulls <- read.csv("../VisInfDocs/Data/distribution_null_model.csv")
# names(nulls)[-1] <- c("alpha1", "alpha2", "beta1", "beta2")
# nulls$Sim <- 1:nrow(nulls)
# nulls$Model <- "M1"
# 
 alt <- read.csv("../VisInfDocs/Data/distribution_jumpTT_model.csv")
# names(alt)[-1] <- c("alpha1", "alpha2", "beta1", "beta2", "beta3")
# alt$Sim <- 1:nrow(alt)
# alt$Model <- "M2"
# 
 alt2nd <- read.csv("../VisInfDocs//Data/distribution_dblpairs_model.csv")
# names(alt2nd)[-1] <- c("alpha1", "alpha2", "beta1", "beta2", "beta4")
# alt2nd$Sim <- 1:nrow(alt2nd)
# alt2nd$Model <- "M3"
# 
# alt2 <- gather(alt, parameter, estimate, 2:6)
# null2 <- gather(nulls, parameter, estimate, 2:5)
# alt2nd2 <- gather(alt2nd, parameter, estimate, 2:6)
# 
# simu <- rbind(alt2[,-1], null2[,-1])
# write.csv(simu, "Data/simulation-1000-M1-M2.csv", row.names=FALSE)


simu2 <- read.csv("../VisInfDocs/Data/simulation-1000-M1-M2-M3.csv")
means <- simu2 %>% group_by(Model, parameter) %>% summarize(
  mean = mean(estimate)
)
library(dplyr)
null_mod_sv <- as.numeric(nulls[,-c(1,6,7)] %>% summarise_each(funs(mean)))
alt_mod_sv <- as.numeric(alt[,-c(1, 7,8)] %>% summarise_each(funs(mean)))
alt_mod2_sv <- as.numeric(alt2nd[,-c(1,7,8)] %>% summarise_each(funs(mean)))
```

Each model, $M1$, $M2$, and $M3$, was fit to the data 1000 times in order to obtain a distribution of parameter estimates. These distributions are shown in Figure \@ref(fig:hist-estimates). Then, the overall mean of the 1000 estimates for each parameter was calculated and used as the plug-in values for simulating from each of the three models. These values are presented in Table \@ref(tab:models). 

\begin{table}[h]
\caption{(\#tab:models) Parameters and estimates of models $M_1$, $M_2$, and $M_3$. Estimates are the mean of 1000 iterations of the model estimates. The lineups that follow are simulated from models using these values.}
\centering
\scalebox{0.8}{
\begin{tabular}{lccrrr}
Effect name & Parameter & Corresponding Statistic & $M_1$  & $M_2$  & $M_3$ \\
\hline
\hline
Rate 1 (wave 1 $\rightarrow$ 2) & $\alpha_1$ & $\sum\limits_{i,j = 1 i\neq j}^n (x_{ij}(t_2) - x_{ij}(t_1))^2 $ & 
`r round(null_mod_sv[1], 2)` &
`r round(alt_mod_sv[1], 2)` & 
`r round(alt_mod2_sv[1], 2)` 
\\
Rate 2 (wave 2 $\rightarrow$ 3) & $\alpha_2$ & $\sum\limits_{i,j = 1 i\neq j}^n (x_{ij}(t_3) - x_{ij}(t_2))^2 $ & 
`r round(null_mod_sv[2], 2)` &
`r round(alt_mod_sv[2], 2)` & 
`r round(alt_mod2_sv[2], 2)`
\\
Outdegree & $\beta_1$ & $s_{i1}(x) = \sum\limits_{j=1}^n x_{ij}$ & 
`r round(null_mod_sv[3], 2)` &
`r round(alt_mod_sv[3], 2)` & 
`r round(alt_mod2_sv[3], 2)`
\\
Reciprocity & $\beta_2$ & $s_{i2}(x) = \sum\limits_{j=1}^n x_{ij}x_{ji}$ & `r round(null_mod_sv[4], 2)` &
`r round(alt_mod_sv[4], 2)` & 
`r round(alt_mod2_sv[4], 2)`
\\
Jumping Transitive Triplets & $\beta_3$ & $s_{i3}(x) = \sum\limits_{\forall j\neq h} x_{ij}x_{ih}x_{hj} \mathbb{I}(v_i = v_h \neq v_j)$ & -- & 
`r round(alt_mod_sv[5], 2)` & -- \\
\# doubly achieved distances & $\beta_4$ & $s_{i4}(x) = |\{j : x_{ij} = 0, \sum\limits_h x_{ih}x_{hj} \geq 2\}|$ & -- & -- &
`r round(alt_mod2_sv[5], 2)`
\end{tabular}}
\end{table}

Because the likelihood function for these complicated models is intractable, `RSiena` implements Markov Chain Monte Carlo simulation to obtain method of moments estimates of the parameter values. This fitting procedure was first introduced in @saompaper. 

The model fitting in `RSiena` is done in three phases. Briefly, in the initial phase, sensitivity of the statistics to the parameter values is determined, then in the second phase, parameter values are fit iteratively. Finally, in the third phase, networks are simulated from the fitted models and the model is checked for convergence [@RSienaManual]. 

The convergence of a non-rate parameter is determined through the simulated values from phase 3 of the algorithm. The simulations are compared to the observed values of the statistics in the data. The values from the simulations should be fairly close to the observed values, but because the fitting is done stochastically, the deviation from statistics will not be exactly zero. Checking for convergence is based on a $t$-ratio of the average of these deviations to the standard deviation of these deviations. `RSiena` also performs an overall maximum convergence check by finding the maximum $t$-ratio value for any linear combination of the observed statistics.  According to the RSiena Manual, "convergence is excellent when the overall maximum convergence ratio is less than 0.2", and for the non-rate parameters, the threshold for "reasonable" convergence is set at 0.3 with excellent convergence when the t-ratio is less than or equal to 0.1 in absolute value. [@RSienaManual]. 

```{r hist-estimates, dependson='read-simu', echo=FALSE, fig.width=8, fig.height=5, out.width='.8\\linewidth', fig.align='center', fig.cap="Histogram of the distribution of model parameters based on 1,000 simulation runs. Model parameter $\\beta_3$ for jumping transitive triplets in model $M_2$ is significantly different from zero, but its inclusion also leads to significant changes in the  other model parameters of model $M_1$. The parameter $\\beta_4$ for doubly acheived distances is also significantly different from zero, but has larger variance. The inclusion of $\\beta_4$ also changes the estimates of the other model parameters, but not as much as the inclusion of $\\beta_3$."}
qplot(data=simu2, estimate, fill=Model, geom="density", alpha=I(0.65)) + facet_wrap(~parameter, scales="free") + theme_bw()
```


## Using the Lineup Protocol

The additional parameters that we included, $\beta_{3}$ and $\beta_4$, were determined to be significant based on a rather simple statistical test, so we wanted to test whether this significance can be detected visually just as simply as the statistical test detects it. If visualizations of simulated networks from two nested models have a much different appearance when placed side-by-side, then the difference in appearance can be attributed to the additional parameter in one. If, however, there is no visually detectable difference, then the additional parameter does not appear to have changed the network structure all that much. Because model selection and diagnostics for network models are less developed areas of the theory, testing network parameters in this visual way could lead to additional methods of model selection for networks. 

### Lineup Simulation

To create the lineups that we used in our experiment, we used the values given in Table \@ref(tab:models) as starting values in simulation of network observations from each of the three models. Each lineup was generated independently of the others. Four different lineup constructions were used: $M1$ v. $M2$, $M2$ v. $M1$, $M1$ v. $M3$, and $M3$ v. $M1$, where $M1$, $M2$, and $M3$ have the objective functions and parameter values given in Table \@ref(tab:models). The lineup types are named to indicate which model is the "null" model and which model is the "alternative" model. The first model listed is considered the null model, so $M-1$ of plots of data simulated from this model are included in the lineup, where $M$ is the total number of plots included in the lineup. Then, one additional plot from the second model listed, the alternative model, is placed among the $M-1$ null plots at random. For each lineup simulated, the same default RSiena algorithm was used that was used to generate the fitted models with the exception that the algorithm only simulated from the given set of parameters. Additionally, each lineup and plot within lineup was generated idependently.

We initially constructed lineups of various sizes, $M \in \{3,6,9,12,16\}$. We ultimately decided to use lineups of size $M=12$ because these lineups appeared to be the most square, and contained enough plots to make the probability of choosing the correct plot at random fairly low, at $\frac{1}{12} = 0.083$ while not overwhelming the viewer with too many plots to examine at once.  

### Parameter Estimation from Lineups

After the lineups were created, we re-fit each of our three models to each panel in each lineup of all sizes. Fitting all models to all lineup plots allows us to gauge the ability of the parameter estimates to provide a measure of lineup identification difficulty. For instance, when comparing models 1 and 2 in a lineup, the estimates from fitting model 2 to plots which were simulated from model 2 should be significantly different from the model 2 estimates from plots simulated from model 1. The smaller the difference between these estimates, the harder it should be to identify the different model in the lineup. 

We did run into convergence problems when estimating parameter values from the lineup data. In about $\frac{1}{3}$ of cases, the algoritm did not converge, as determined by phase 3 of the fitting process and the convergence values stated in Section \@ref(fitting). Convergence by the model that was fit to the data and the model from which the data were simulated is sumarized in Figure \@ref(fig:convergence). Additionally, distributions of parameter values for all fitted models according to the fitted model, the model from which the data were simulated, and whether or not the estimation converged are shown in Figure \@ref(fig:convergence-ests). For the estimates of $M1$ that did not converge, the distributions of the estimates were very different from what they should have been, i.e. distributions of $\hat{\beta}_3$ and $\hat{\beta}_4$ should be near 0 when the true model was model $M1$, but this is not the case for instances where the algorithm did not converge. 

```{r convergence, fig.width=8, fig.height=3, echo = FALSE, warning = FALSE, fig.cap="Pattern of convergence. A total of 67.7\\% of all lineup data converged in 5,000 iterations. The simplest model, $M_1$ has the highest rate of convergence. For true models $M_2$ and $M_3$, data generated from the true model converged at a higher rate than data generated from the other model."}
load("../VisInfDocs/Data/lus_ests_truth.rda")
lus_ests_truth$param_name <- 
  factor(lus_ests_truth$param_name,
    levels = c("rate", "outdegree (density)", "reciprocity",
               "transitive triplets jumping alcohol2",
               "number pairs at doubly achieved distance 2"))        

ggplot(data=lus_ests_truth) +
  geom_bar(aes(x=model, fill=convergence), position="fill", alpha = 0.6) + 
  facet_grid(.~true_model, labeller="label_both") +
  theme_bw() +
  theme(legend.position="bottom") + ylab("Ratio") +
  scale_fill_brewer("Convergence", palette="Set1") +
  xlab("Estimated Model")
```

```{r convergence-ests, fig.cap="Difference between parameter estimate and true value. Panels with a light grey background show model fits with data sampled from the same model. Densities are drawn for both converged and non-converged data. The red-filled densities should have a mode near zero, idicating that the model converged toward the correct value. For data from model $M_1$, estimates for parameters $\\beta_3$ and $\\beta_4$ converge to a wrong value.", fig.width=8, fig.height=8, echo = FALSE, warning = FALSE}
load("../VisInfDocs/Data/lus_ests_truth.rda")
lus_ests_truth$param_name <- 
  factor(lus_ests_truth$param_name,
    levels = c("rate", "outdegree (density)", "reciprocity",
               "transitive triplets jumping alcohol2",
               "number pairs at doubly achieved distance 2"))        
lus_ests_truth$model_label <- lus_ests_truth$model
lus_ests_truth$true_model_label <- sprintf("True~Model:~%s", lus_ests_truth$true_model)
lus_ests_truth$param_label <- c("alpha", "beta[1]", "beta[2]", "beta[3]", "beta[4]")[as.numeric(lus_ests_truth$param_name)]
ggplot(data=lus_ests_truth %>% 
         filter(
           !is.na(convergence),
           (true_model=="M3" & model != "M2") | (true_model=="M1") |
             (true_model=="M2" & model != "M3")
           )) +
  geom_rect(xmin=-25, xmax=25, ymin=-0.5, ymax = 1, fill=rgb(.25,.25,.25,alpha=.25), data=unique(subset(lus_ests_truth, (true_model==model) & (!is.na(param_est)) )[,c("true_model_label", "model_label", "param_label")]) ) +
  geom_vline(xintercept = 0, colour="grey50") +
  geom_density(aes(x=param_est-true_value, fill=convergence), 
               alpha= 0.6) +
  facet_grid(model_label+param_label~true_model_label, drop=TRUE, labeller=label_parsed) +
  theme_bw() +
  theme(legend.position="bottom", 
        strip.text.y = element_text(angle=0)) + 
  scale_fill_brewer("Convergence", palette="Set1") +
  xlab("Difference between parameter estimate and true value") + 
    xlim(c(-15,15))
```

```{r comparison, fig.width=8, fig.height=6, out.width='\\linewidth', echo = FALSE, warning = FALSE, fig.cap = "Comparison of model estimates under all three models under investigation."}
l1 <- levels(lus_ests_truth$param_name)
l1 <- stringr::str_trim(gsub("2","", l1))
l1[5] <- "#pairs at doubly achieved distance"

# subset on ones that are actually converged. XXX No - that kills the zeroes in estimates that should be zero
ggplot(data = subset(lus_ests_truth)) + #, convergence=="Converged")) + 
  geom_vline(aes(xintercept = 0), colour="grey50") +
  geom_density(alpha = .5, aes(x = param_est, fill = param_name)) + 
  facet_grid(model~true_model, scales = 'free', 
             labeller = "label_both") + xlim(c(-20,20)) +
  theme_bw() + 
  scale_fill_brewer("Parameter", palette="Dark2",
     labels=c(bquote(paste(alpha,": ", .(l1[1]), sep="")), 
              bquote(paste(beta[1],": ", .(l1[2]), sep="")),
              bquote(paste(beta[2],": ", .(l1[3]), sep="")),
              bquote(paste(beta[3],": ", .(l1[4]), sep="")),
              bquote(paste(beta[4],": ", .(l1[5]), sep="")))) +
  theme(legend.position = "bottom") +
  xlab("Parameter Estimate") + 
  guides(fill = guide_legend(nrow = 3, byrow = TRUE))
```

```{r est, echo = FALSE}
#For model $M_2$, parameter $\beta_3$ is estimated to be significantly different from zero in almost all cases (XXX how many exactly? XXX).

# get beta3s where true model and fit model are M2
beta3sig <- lus_ests_truth %>% 
  filter(true_model == "M2", model == "M2", 
         param_name == "transitive triplets jumping alcohol2")
convergebeta3 <- length(which(beta3sig$convergence == "Converged")) / nrow(beta3sig)

# get ones that converged.
beta3sig2 <- beta3sig %>% 
  filter(convergence == "Converged") %>%
  mutate(tstat = param_est/param_est_se,
         ttestpval = 2*dt(tstat, df = 1))
beta3sig05 <- sum(beta3sig2$ttestpval <= 0.05) / nrow(beta3sig2)
beta3sig10 <- sum(beta3sig2$ttestpval <= 0.10) / nrow(beta3sig2)
```

Figure \@ref(fig:comparison) shows an overview of model estimates for each simulated data set. What we expect to see is that estimates do not change much if they are estimated under a model different from the one they are generated from. This is true for all data sets estimated under model $M_1$ independently of which model they were simulated from (see top row of Figure \@ref(fig:comparison). For data fit under model $M_2$ we see that parameter $\beta_3$ is estimated to be about zero, if the data is generated from models $M_1$ or $M_3$.
For model $M_2$, the parameter $\beta_3$ is estimated to be significantly different from zero in `r round(100 * beta3sig10, 1)`\% of cases. This coincides with our expectation.

However, the bottom row of Figure \@ref(fig:comparison) shows that independently of which model data is generated from, $\beta_4$ is estimated to be significantly different from zero in a large number of cases (about half of the data generated from model $M_2$ and more than that in model $M_1$). While $\beta_4$ is a highly significant parameter in model $M_3$, this questions the way that parameters are fitted and tells us that we are not likely to be able to visually distinguish between data generated from model $M_3$ and data generated from models $M_1$ and $M_2$. We might, however, be able to distinguish between data sets for which $\beta_4$ is estimated to be significantly different from zero and those where $\beta_4$ is not significant. We hypothesize that the strong influence of the inclusion of $\beta_4$ in $M3$ makes the probability of being able to visually distiguish $M1$ from $M3$ extremely low. The $\beta_4$ estimate is always significantly greater than zero in fitted models that converged. Thus, $\beta_4$ should have been included from the beginning.

```{r lusparms, echo=FALSE, fig.width=8, fig.height = 8, out.width='.6\\linewidth', fig.align = 'center', warning = FALSE, fig.cap="Scatterplots of smallfriends and smallfriends-rev: comparing the ratio of null plots with smaller estimates of $\\beta_3$ in the nulls than in the data panel (top row) and larger estimates of $\\beta_3$ in the nulls than in the data panel. In lineups with a ratio of 1 the data should be 'easy' to identify in both scenarios."}
load("../VisInfDocs/Data/lus_ests_truth.rda")

lus_ests_truth$true_panel <- lus_ests_truth$true_model == lus_ests_truth$model
lus_spread <- tidyr::spread(lus_ests_truth[,c("lineupid","model", "true_model", "panel_num", "param_name", "param_est")], model, param_est)

# parameters should be most different between data plot and null plots

# merge in true values:
truth <- unique(subset(lus_ests_truth, true_panel==TRUE)[,c("lineupid", "true_model","param_name","true_value")])

lus_spread <- merge(lus_spread, truth, by=c("lineupid","true_model", "param_name"), all.x=TRUE)
lus_spread$true_value[is.na(lus_spread$true_value)] <- 0

# compute a distance between the data panel and the closest null panel
lus_spread$part <- gsub("(.*)-[0-9]*-[0-9]*", '\\1', lus_spread$lineupid)

lus_summary_2 <- lus_spread %>% dplyr::group_by(part, lineupid, param_name) %>%
  dplyr::summarize(
    data_n = sum(true_model=="M2"),
    data_est = mean(M2[true_model=="M2"]),
    data_sd = sd(M2[true_model=="M2"], na.rm=TRUE),
    data_dist = data_est - max(M2[true_model=="M1"]),
    upper_qu = sum(data_est > M2[true_model=="M1"]) / (n()-1),
    data_panel = mean(panel_num[true_model=="M2"])
)

p1 <- qplot(data_dist, upper_qu, data = subset(lus_summary_2, !is.na(upper_qu) & param_name=="transitive triplets jumping alcohol2")) +
  facet_grid(.~part) + xlab("Difference in M2 estimate of data panel and maximum of the null panels (from M1)") +
  ylab("Ratio of null panels with smaller M2 estimate") +
  ggtitle("jumping transitive triplets")


lus_summary_2b <- lus_spread %>% dplyr::group_by(part, lineupid, param_name) %>%
  dplyr::summarize(
    data_n = sum(true_model=="M1"),
    data_est = mean(M2[true_model=="M1"]),
    data_sd = sd(M2[true_model=="M1"], na.rm=TRUE),
    data_dist = data_est - min(M2[true_model=="M2"]),
    lower_qu = sum(data_est < M2[true_model=="M2"]) / (n()-1) 
)

p2 <- qplot(data_dist, lower_qu, data = subset(lus_summary_2b, part %in% c("smallfriends", "smallfriends-rev") & param_name=="transitive triplets jumping alcohol2")) +
  facet_grid(.~part) + xlab("Difference in M2 estimate of data panel and minimum of the null panels (from M2)") +
  ylab("Ratio of null panels with larger M2 estimate") +
  ggtitle("jumping transitive triplets")

#gridExtra::grid.arrange(p1,p2)
```

```{r jttss}
jtts <- read.csv("../VisInfDocs/Data/lineups-jtt.csv")
jtts$lineupid <- with(jtts, paste(model, m, rep, sep="-"))
jtts$data_panel <- jtts$plot_order
lus_summary_2 <- merge(lus_summary_2, jtts[, c("lineupid", "jtt", "data_panel")], by=c("lineupid","data_panel"), all.x=TRUE)
#qplot(data=subset(lus_summary_2, param_name=="transitive triplets jumping alcohol2" & part=="smallfriends"), data_est, jtt, geom="jitter") + geom_label(data=subset(lus_summary_2, param_name=="transitive triplets jumping alcohol2" & jtt > 20 & part=="smallfriends"), aes(label=lineupid), alpha=.6)
```

```{r lusparms-eff2, echo=FALSE, fig.width=8, fig.height = 8, out.width='.6\\linewidth', fig.align = 'center', warning=FALSE, eval = FALSE}
lus_summary_3 <- lus_spread %>% group_by(part, lineupid, param_name) %>%
  summarize(
    data_n = sum(true_model=="M3"),
    data_est = mean(M3[true_model=="M3"]),
    data_sd = sd(M3[true_model=="M3"], na.rm=TRUE),
    data_dist = min(M3[true_model=="M1"]) - data_est,
    lower_qu = sum(data_est < M3[true_model=="M1"])/(n()-1)
  )

p1 <- qplot(data_dist, lower_qu,  data = subset(lus_summary_3, !is.na(lower_qu) & param_name=="number pairs at doubly achieved distance 2")) +
  facet_grid(.~part) +
  xlab("Difference in M3 estimate of data panel and minimum of the null panels (M1 nulls)") +
  ylab("Ratio of null panels with larger M3 estimate") +
  ggtitle("doubly achieved distance")

lus_summary_3b <- lus_spread %>% group_by(part, lineupid, param_name) %>%
  summarize(
    data_n = sum(true_model=="M1"),
    data_est = mean(M3[true_model=="M1"]),
    data_sd = sd(M3[true_model=="M1"], na.rm=TRUE),
    data_dist = max(M3[true_model=="M3"]) - data_est,
    upper_qu = sum(data_est > M3[true_model=="M3"])/(n()-1)
  )

p2 <- qplot(data_dist, upper_qu,  data = subset(lus_summary_3b, !is.na(upper_qu) & param_name=="number pairs at doubly achieved distance 2" &
part %in% c("smallfriends-eff2", "smallfriends-eff2-rev"))) +
  facet_grid(.~part) +
  xlab("Difference in M3 estimate of data panel and minimum of the null panels (M3 nulls)") +
  ylab("Ratio of null panels with smaller M3 estimate") +
  ggtitle("doubly achieved distance")

gridExtra::grid.arrange(p1,p2)
#p1
```

### Results from A Pilot Study

We performed a pilot study that consisted of an evaluation of a set of 20 lineups by 11 volunteers. For each of the model situations ($M_1$ vs $M_2$, $M_2$ vs $M_1$, $M_1$ vs $M_3$ and $M_3$ vs $M_1$) one lineup of size $m = 3, 6, 9, 12$ was used. Overall, the number of data identifications in the lineups was very low (34 out of 220 evaluations). Participants identified the data plot in two to four of the lineups they evaluated. The mode was three data identifications out of twenty per participant. 

\paragraph{Suitability of $M_3$ in lineups:}
```{r data-picks, echo=FALSE, fig.align='center', fig.width=10, fig.height=5, out.width='0.85\\linewidth', fig.cap="Barchart summarizing the number of responses from the pilot study by model and lineup. Color shows the number of times the data panel was chosen from the lineup. Clearly, the first two sets of lineups ($M_1$ vs $M_2$ and $M_2$ vs $M_1$) have on average higher number of data identifications." }
pdffiles <- dir("../VisInfDocs/GGExperimentApr28/", pattern="pdf")
pdffiles <- gsub(".pdf","",pdffiles)
#pdffiles <- gsub("-m","",pdffiles)
#pdffiles <- gsub("-rep","",pdffiles)

dframe <- strsplit(pdffiles, split=" ") %>% plyr::ldply(function(x) x)
names(dframe) <- c("stimulus", "lineupid")

results <- read.csv("../VisInfDocs/GGExperimentApr28/responses_GGExpApr28.csv")
lus <- results %>% group_by(Lineup, ChosenLU) %>% summarize(
  tally = n(),
  data = Answer[1],
  reason = paste(Reasoning, collapse="|")
)
names(lus)[2] <- "panel"

dframe_res <- merge(dframe, lus, all=TRUE, by.x="stimulus", by.y="Lineup")
dframe_res$data_pick <- with(dframe_res, data==panel)
dframe_res$model <- factor(gsub("(.*)-m.*", "\\1", dframe_res$lineupid))
levels(dframe_res$model) <- c("M1 vs M2",  "M1 vs M3", "M3 vs M1", "M2 vs M1")
dframe_res$model <- factor(dframe_res$model, levels = c("M1 vs M2",  "M2 vs M1", "M1 vs M3", "M3 vs M1"))
dframe_res$label <- gsub(".*-(m.*)", "\\1", dframe_res$lineupid)

dframe_res$data_pick <- factor(dframe_res$data_pick, levels=c("TRUE", "FALSE"))
dframe_res$res <- factor(dframe_res$data_pick, levels=c("TRUE", "FALSE"))
qplot(label, weight=tally, fill=data_pick, data=dframe_res) +
  facet_grid(.~model, scales="free", space="free") + theme_bw() +
  theme(axis.text.x = element_text(angle=45, hjust=1)) + 
  scale_fill_brewer("Data panel chosen", palette="Set1", na.value="grey80") + xlab("Lineup")
```

Figure \@ref(fig:data-picks) shows barcharts of responses from the pilot study detailing the number of data identifications in each lineup. It is more difficult to identify the data plot in lineups of graphs based on data from models $M_1$ and $M_3$ than  in lineups of graphs based on data from models $M_1$ and $M_2$.

## Amazon Turk Experiment

### Methods 

In order to test our hypotheses, we set up an Amazon Mechanical Turk experiment [@turk] using the lineup protocal of @Bujaetal. We presented four types of lineups:  We created a total of 25 lineups to show to Amazon Turk users taking our experiment: 10 for M1 v. M2 and 5 each for the other three types of lineups. In each lineup, there were 12 plots shown: 11 of the plots were simulated from the first model (the "null" model) and 1 was simulated from the second model (the "alternative" model). We chose to show lineups of size 12 because we felt that showing more than 12 would be too large of a cognitive load, while showing fewer than 12 would leave too much of the experiment to random chance. The order of the plots within the lineups was randomly assigned. We selected five lineups presented from each group based on the following criteria: first, for M1 v M2 and M1 v M3, we selected the lineups where the additional parameter in the objective function, $\beta_3$ and $\beta_4$, respectively, had the largest estimated value among the 12 networks presented. There were not many of these in the size 12 lineups, so our other selection criteria was that the estimated parameter value of interest was larger in the alternative panel than in at least half of the other lineups, and that it was close to the estimate from the panel that did have the largest value. For the $M2$ v $M1$ and $M3$ v $M1$ lineups, we first selected lineups of size 12 where the smallest parameter estimate belonged to the alternative model. If more lineups were needed, we next selected those which had estimates *smaller* than at least half of the other panels and were close to the *minimum* estimate in the lineup. For the remaining five plots of type $M1$ v $M2$, we chose the lineups where the alternative plot had the largest number of jumping transitive triplets, i.e. with the highest value of $\sum_i s_{i3}(x)$, appear in the network. We chose this statistic because its value has great effect on both the estimation of the $\beta_3$ parameter and on the visual appearance of the plot.

In order to become a subject in our experiment, the Amazon turk user had to first read through some introductory material and prove they could identify the correct plot in two test lineups. Each user was greeted with a brief welcome message and experiment description, seen in Figure \@ref(fig:directions). Then, a training page about how to identify the correct plot in a lineup, seen in Figure \@ref(fig:lineupex) was present. After the training page, two trial plots were presented. In order to complete the experiment, the user had to correctly identify the alternative plot. In the trial they identified a plot, provided reasoning for this choice (most complex, least complex, or other), and provided their confidence level in their choice (Very Uncertain, Uncertain, Neutral, Certain, Very Certain). Two of these trial plots with the correct responses selected are shown in Figure \@ref(fig:lineuptrial) one lineup of type $M1$ v. $M2$ and one of type $M2$ v. $M1$. The training lineups shown to the Turkers were randomly selected from ten lineups that were constructed to be very simple for training purposes. Once the turk user chose the correct plot in the two trial plots, the experiment proceeded with the same interface as the trial plots, with users selecting which plot they thought was most different, why they thought that, and how certain they were of their choice of different plot for 10 lineups chosen at random from the aforementioned pool of 25. The users were also allowed to select multiple plots for their response. 

```{r directions, out.width="50%", fig.height=3, fig.align='center',  fig.cap="The welcome message shown to the Amazon Turk users."}
knitr::include_graphics(c("VisInfDocs/Results/Directions"))
```

```{r lineupex, out.width="50%", fig.cap="The first training page in the Amazon Turk experiment.", fig.show='hold'}
knitr::include_graphics(c("VisInfDocs/Results/Ex1","VisInfDocs/Results/Ex2"))
```

```{r lineuptrial, out.width="50%", fig.cap="Two of trial plots that users had to correctly answer in order to participate in the Amazon Turk experiment. The lineup on the left is representative of the $M1$ v $M2$ type of lineup, while the lineup on the right is representative of the $M2$ v $M1$ type.", fig.show='hold'}
knitr::include_graphics(c("VisInfDocs/Results/Trial1","VisInfDocs/Results/Trial2"))
```

### Results

```{r get-res, echo=FALSE, warning=FALSE, message=FALSE, fig.align="center", fig.cap="Boxplot showing the percent of users correctly choosing the alternative model in all 25 lineups. In 15 out of 25 lineups, no user picked the correct plot. The maximum was 64.00\\%, and the second highest was 35.29\\% correct.", fig.height=3}
ids <- readr::read_csv("https://raw.githubusercontent.com/erichare/lineups/f19635a00507210013ba9e631761ad0c89a6564d/experiments/turk21/details/picture-details.csv")
tab <- read.csv("../VisInfDocs/Results/turk21_users.csv")
tab2 <- unique(tab)
#length(unique(tab2$nick_name))
#head(sort(table(tab2$nick_name),decreasing = T))
#tab2 %>% filter(nick_name == "A3U3L1XQVR362A")
#tab2 %>% filter(nick_name == "ADTIO3A6TM9CG")
#tab2 %>% filter(nick_name %in% names(table(res$nick_name))[table(res$nick_name) != 10])
res <- read.csv("../VisInfDocs/Results/turk21_feedback.csv", stringsAsFactors = F)
# remove people who only completed 1 plot
res <- res %>% filter(!(nick_name %in% names(table(res$nick_name))[table(res$nick_name) != 10]))
# need to repeat rows with weights for people who selected multiple plots.
res_norpt <- res[-grep(",", res$response_no),]  
res_rpt <- res[grep(",", res$response_no),]  

library(stringr)
# count # of responses (ttr = times to repeat)
ttr <- str_count(string = res_rpt$response_no, pattern = ",") + 1
rpt_rows <- rep(1:nrow(res_rpt), ttr)
rpt_rows2 <- res_rpt[rpt_rows,]
rpt_rows2$count_response <- unlist(sapply(ttr, seq_len))
rpt_rows2$weight <- 1/rep(ttr, ttr)
split_rows <- strsplit(rpt_rows2$response_no, ",")
rpt_rows2$tot_response <- rep(ttr, ttr)
rpt_rows2$response_no2 <- NA
for(i in 1:nrow(rpt_rows2)){
  rpt_rows2$response_no2[i] <- split_rows[[i]][rpt_rows2$count_response[i]]
}

res_norpt$count_response <- 1 
res_norpt$weight <- 1
res_norpt$response_no2 <- res_norpt$response_no
res_norpt$tot_response <- 1 

# res with weight column   
new_res <- rbind(res_norpt, rpt_rows2)

#intersect(names(ids), names(res))
res2 <- left_join(new_res, ids, by = 'pic_id')
res2$correct <- res2$response_no == res2$obs_plot_location
res2$time <- res2$end_time - res2$start_time
res2$lineup_name <- as.factor(paste(res2$test_param, res2$param_value))
# with the correct labels (in order of data plot w/value most different from rest to least different)
more_details <- read.csv("../VisInfDocs/experimentdetails_Aug31.csv", stringsAsFactors = F)
res3 <- left_join(res2, more_details, by = c("data_name" = "lineup_filename"))
# higest jtts are: reps 18, 20, 6, 19, 3 with 
              # jtts of 40, 24, 18,17, 9
res3$group_name <- paste0(res3$group, " #", res3$group.rep)

# get the weights
res3 %>% group_by(group_name, response_no2, correct) %>% summarise(tot_wt = sum(weight)) -> summ_res_wts

# res3 %>% group_by(group_name) %>% 
#   summarize(total = n(), tot_correct = sum(correct), perc_correct = tot_correct/total) %>% 
#   arrange(desc(perc_correct)) -> lineup_by_perc_correct
#lineup_by_perc_correct[which.max(lineup_by_perc_correct$total),]
#lineup_by_perc_correct[which.min(lineup_by_perc_correct$total),]
# box plot no - ew. 
# ggplot(data = lineup_by_perc_correct, aes(x = 1, y = perc_correct)) + geom_boxplot() + 
#   scale_y_continuous(breaks = seq(0, 1, .10), labels = paste0(seq(0, 1, .10)*100, "%")) + 
#   coord_flip() + theme_classic() + 
#   theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) + 
#   labs(x = "", title = "Percent of Users Correctly Identifying Alternate Model", y = "")
```

We collected on ten lineups each from 77 Amazon Turk users. A table of demographic information collected on the participants in the experiment is in Table \@ref(tab:demos). 
```{r demos}
t1 <- t(data.frame(table(tab2$gender)))
rownames(t1) <- c("Gender:", "Count:")
colnames(t1) <- t1[1,]
t1 <- t1[2,]
t1[as.numeric(t1) < 5] <- "*"
t3 <- t(data.frame(table(tab2$academic_study)))
rownames(t3) <- c("Highest Education:", "Count:")
colnames(t3) <- t3[1,]
t3 <- t3[2,]
t3[as.numeric(t3) < 5] <- "*"
t2 <- t(data.frame(table(tab2$age)))
rownames(t2) <- c("Age:", "Count:")
colnames(t2) <- t2[1,]
t2 <- t2[2,]
t2.2 <- t2
t2.2 <- t2.2[1:7]
names(t2.2)[6:7] <- c("46-55", "Over 55")
t2.2[6:7] <- c(sum(as.numeric(t2[6:7])), sum(as.numeric(t2[8:9])))
knitr::kable(
  list(
    t1,
    t2,
    t3
  ),
  caption = 'Demographic information collected from experiment participants. An asterisk (*) indicates fewer than 5 participants.', booktabs = TRUE
)
```

Because of the random assignment of the 25 plots to users, almost every lineup was seen by a different number of people. Repetition 3 of $M1$ v. $M3$ was seen by 47 different users while repetition 2 of the same type was seen by only 6 different users. The mean number of times seens was 30.8 per lineup and the median was 31. In 15 of the 25 lineups, no user identified the correct plot, and in the remaining 10 lineups, the users correctly identified the alternative plot in the lineup a minimum of 4.35\% of the time and a maximum of 64\% of the time. A plot showing the number of different plots chosen, how many times they were chosen, and whether or not it was the correct choice is given in Figure \@ref(fig:facets-correct). Since the turk users were allowed to select multiple plots, the values shown in the histogram are actually the total weights received for each plot in the lineup. If the user selected only one plot, then that plot received a weight of 1, but if the user selected $n \geq 2$ plots, then each plot selected received a weight of $\frac{1}{n}$. 

```{r facets-correct, echo=FALSE, fig.align="center", fig.cap="Plots chosen for each of the 25 lineups in the experiment.", fig.height=9}
ggplot(data = summ_res_wts, 
       aes(x = reorder(as.factor(response_no2), -tot_wt), 
                                y = tot_wt,
                                fill = correct)) + 
  geom_bar(color = 'black', stat = 'identity') + 
  scale_fill_manual(values = c("white", "grey30"), 
                    name = "Correct?") + 
  facet_wrap(~group_name, scales = "free", nrow = 5) +
  theme_bw() + 
  theme(legend.position = 'bottom') + 
  labs(x = "User Response" , y = "Number of Responses") 
```

This plot shows us the abysmal ability of the Turkers to identify the alternative plot correctly. There are also a few other interesting results from this plot. First, there are several lineups where a majority of participants selected the same wrong alternative plot. Additionally, we see a lot of variation in the number of different plots selected at the alternate plot by the Turkers. At most, there were 10 different plots selected, compared to 2 at the opposite end.

Next, we investigate the confidence of the experiment's participants in selecting the most different plot from the others. Overall, in 40\% of the responses, the respondent was certain they were correct. User confidence in the remaining categories, neutral, uncertain, very certain, and very uncertain, was 26.88\%, 16.88\%, 9.48\%, and 6.76\%, respectively. These results are summarized by lineup in Figure \@ref(fig:cert-plot). We also performed a 5-sample test for equality of proportions in order to test the null hypothesis that the proportion of correct responses is the same in all 5 certainty categories. This test resulted in a $p$-value of 0.5881, so there is no evidence that the certainty of a user's response affects whether or not they choose the correct plot. This provides further evidence that visually detecting differences between networks simulated from different models is an extremely difficult problem. 

```{r cert-plot, echo=FALSE, warning = FALSE, message = FALSE, fig.align='center', fig.height=4, fig.cap="All responses for all lineups, separated by whether the plot selected was the true alternative plot (TRUE) or not (FALSE) and colored by the respondent's uncertainty levels. We see here that most respondents were certain in their answer whether or not they were correct."}
res2$conf_level <- ordered(res2$conf_level, levels = c("Very Uncertain", "Uncertain", "Neutral", "Certain", "Very Certain"))

ggplot(data = res3, aes(x = group_name)) + 
  geom_bar(aes(fill = conf_level), color = 'grey60', position = 'fill') + 
  scale_fill_brewer(palette = "RdBu") + 
  coord_flip() + 
  facet_wrap(~correct) + 
  theme_bw() + labs(x = "Lineup Name") 

summary.conf <- data.frame(table(res2$conf_level, res2$correct))
summary.conf %>% tidyr::spread(Var2, Freq) -> summary.conf
rownames(summary.conf) <- as.character(summary.conf[,1])
names(summary.conf) <- c("Var1", "Failures", "Successes")
summary.conf <- summary.conf[,-1]
summary.conf <- summary.conf[,c(2,1)]
prop_test_for_conf_corr <- prop.test(as.matrix(summary.conf))
```

We next investigate the length of time that users' took to respond to each lineup. The minimum was 3.762 seconds and the maximum was 578.8 seconds (nearly 10 minutes). The median was 13.01 seconds and the mean was 20.32 seconds. First, a 2-sample Kolmogorov-Smirnov test for equality of distribution was done to see if the distribution of times for correct plots chosen is the same as the distribution of times for incorrect plots chosen. This two-sided test resulted in a $p$-value of 1, so there is no evidence that the distribution of times is different whether or not the response was correct. 

```{r time-plot, echo=FALSE, fig.height=4, out.width="75%", fig.cap="Log of time taken to complete a lineup for all responses, separated by whether or not the response was correct. The distribution is roughly the same for correct and incorrect responses, with more right skew on incorrect responses."}
#qplot(x = res2$time, binwidth = 2)
#ggplot(data = res2, aes(x = conf_level, y = log(time))) +
#  geom_boxplot() + coord_flip() + theme_bw()
#ggplot(data = res2, aes(x = correct, y = log(time))) + geom_boxplot() + coord_flip() + theme_bw()
ggplot(data = res2, aes(x = log(time))) + 
  geom_histogram(aes(y = ..density..),binwidth = .5, fill ='white', color = 'black') +
  #geom_density(color = 'red', alpha = .8) +
  facet_wrap(~correct, nrow = 2, scales = 'free_y', labeller = label_both) + theme_bw()
# ggplot(data = res2, aes(x = lineup_name, y = log(time))) + 
#   geom_boxplot() + 
# #  facet_wrap(~conf_level) + 
#   coord_flip() + 
#   theme_bw() + 
#   labs(y = "Time (in seconds)", x = "Lineup Name")
# 2-sample kolmogorov-smirnov test for equality of distributions
# ks.test(x = summary(res2$time[res2$correct]), y = summary(res2$time[!res2$correct]))
# fail to reject null
```

### Discussion

This small experiment suggests that detecting a network model difference from just one simulation from one model and 11 from the other model is just about impossible. This result is fairly unsurprising: drawing a single random point from, say, a $\chi^2_1$ distribution and placing it in a lineup with 11 separate draws from a standard normal distribution would likely have similar results. Every once in a while, a chi-square value would appear that would be too large to belong with draws from a standard normal distribution, but values near 1, the mean, would probably not appear that different from some random standard normal draws. This means that we need to develop a different way to compare two network models. We need a way to visualize many samples from a network model in one panel. 

We have also discovered that a high value of the JTT statistic leads to correct lineup selection more than a high estimated parameter value. This leads us to question whether statistical significance has any relation to visual detectability at all.  

## Future Work

The next part of this paper requires several more experiments to be performed. We will simulate data from many more models, each with different parameters and parameter values included in the model. Then, networks simulated from null models like $M1$ will be placed in lineups with a network simulated from one of the several alternative models we will define. By performing visual inference on SAOMs in this way, we hope to determine which parameters, when included, cause significant change in the visible network structure. 

## Goodness of Fit Testing 

Visual inference can also be applied to network models to perform goodness of fit tests. We choose a model for the null hypothesis that we think generated the data and simulate several networks from the model. In a lineup, we place a plot of the data at random among plots of the null data. If the data plot is not easily identifiable among the null plots, then there is evidence that the model is a good fit. However, if the data plot sticks out like a sore thumb, then there is evidence that the model is not apporpriate for the data. A proof-of-concept example using the small friendship network data from section \@ref(smallfriends) is presented in Figure \@ref(fig:coolex). In this lineup, the true second network observation is placed at random among five networks simulated from M1 as presented in section \@ref(m1m2).

```{r coolex, fig.cap="The data placed among five networks simulated from model 1. Observers would be prompted to select the plot that looks the most different from the others."}
whats <- read.csv("BlindfoldDocs/1000_sims_m1.csv", stringsAsFactors = F)
set.seed(1234567)
plots <- sample(1000, 5)
lineupdat <- whats %>% filter(count %in% plots)
nulldat <- lineupdat[,-1]
datdat <- actual2
datdat <- datdat[,1:2]
datdat$count <- 1001
names(datdat) <- c("from", "to", "count")
exlu <- rbind(nulldat, datdat)
set.seed(98765)
exlu$plot_order <- rep(sample(6), as.vector(table(exlu$count)))
ggplot(data = exlu) + 
  geom_net(aes(from_id = from, to_id = to), fiteach = T, size = 2, linewidth = .5) + 
  facet_wrap(~plot_order) + 
  theme_net() + theme(panel.background = element_rect(fill = NA, colour = 'black'))
```

The true data in figure \@ref(fig:coolex) is in panel $2^2-1$. To compare, we have also simulated five networks from model M2 and placed the true data at random in a lineup with them. This lineup, shown in Figure \@ref(fig:coolex2), is obviously not independent of the first lineup, in Figure \@ref(fig:coolex), because the true data are shown in both. However, it is compelling to see the similarities between the 5 networks simulated from M2 and the true data. The true data in Figure \@ref(fig:coolex2) are in panel 5, but even knowing what the true data look like from Figure \@ref(fig:coolex), it is difficult to pick it out, especially because of the similarities between panels 3 and 5: they are almost identical! 

```{r coolex2, fig.align='center', fig.cap="The true data placed at random among five simulations from M2."}
set.seed(1234567)
plots <- sample(20, 5)
m2sims <- read.csv("VisInfDocs/Data/m2sims20.csv", stringsAsFactors = F)
lineupdat <- m2sims %>% filter(count %in% plots)
nulldat <- lineupdat[,-1]
names(nulldat) <- c("from", "to", "count")
exlu <- rbind(nulldat, datdat)
set.seed(34925)
exlu$plot_order <- rep(sample(6), as.vector(table(exlu$count)))
ggplot(data = exlu) + 
  geom_net(aes(from_id = from, to_id = to), fiteach = T, size = 2, linewidth = .5) + 
  facet_wrap(~plot_order) + 
  theme_net() + theme(panel.background = element_rect(fill = NA, colour = 'black'))
```

In an experiment setting, the true data cannot be shown more than once any one participant, so these lineups could not both be shown as they are here. 

## Why Visualize?

The purpose of these visualization methods is to complement exististing statistical tests. Residual plots help demontstrate goodness-of-fit of linear models because an ANOVA table on its own does not tell the whole story. The accompanying graphics provide more information in a compact space; the p-value alone is not enough. 