# Literature Review {#litreview}

## Seeing the Forest Before Looking at Trees {#intro}

Social networks have been studied for decades, beginning with a few foundational works, the most well known of which is the 1967 study, "The Small World Problem" by Stanley Milgram (@goldenberg09). But in recent years, the study of social networks has grown wildly in popularity due to an increase in the availability of and easy of access to social network data. The digital revolution has led to the creation of social media, linking people from all over the world in a way we never have been before. Now that platforms like Facebook, Twitter, and LinkedIn permeate our world, just about everyone knows what social networks are. In academic circles, collaboration networks are a type of social network that have been extensively studied and can even be a point of pride, like a mathematician's Erd&#x00F6;s  number (@erdos). Social networks are a rich source of knowledge, but the data format does not fit easily within traditional data collection paradigms. Traditionally, data collection involves a set of units of the same, or at least similar, kind, on which observations are made. The storage of traditional data is simple and organized: rows contain variable values collected from units. These units can be people, plants, animals, stocks, objects, fields, and anything else under the sun, but one social network consits of many units, yet on the whole is just one observation. When observing a social network, one observes the possibly very numerous actors (also referred to as vertices or nodes) and the relationships (also referred to as edges or ties) between those actors. One can also collect information on the nodes and the edges separately, such as the age or gender of people and the length of their relationship or how strong it is in a friendship network. Thus, information on the entire network is more difficult to store than traditional data with which statisticians usually work. 

This apparent difficulty has not stopped researchers in many different fields from studying social and other types of networks. Sociologists work with human relationship networks of all kinds imaginable, biologists work with protein-protein interaction networks, neurologists use fMRI scans to study biologic neural networks, and the list goes on. These disciplines worked separately for many years, each developing their own measures, softwares, and theories about the fundamental properties of networks. And although statisticians were comparatively late to the party, many statistical models exist for network analysis. Beginning with the classic Erd&#x00F6;s-R&#x00E9;nyi random graph model and varying in structure, complexity, and application to include longitudinal network data, such as continuous time markov chain models (@goldenberg09). The many varying models that exist just for social network analysis are impressive, but I focus my research on one type of continuous time markov chain (CTMC) models, called Stochastic Actor-Oriented Models (SAOMs). A full introduction to the various models that exist for social network analysis is presented in Section \@ref(models), and a full introduction to the structure and theory of SAOMs is presented in Section \@ref(saoms). 

## Statistical Models for Social Networks {#models}

The literature on statistical models for networks is extensive. In their thorough "Survey of Statistical Models", Goldenberg et al separate these models in to two primary classes: static and dynamic. I discuss the several types of models in each of these two categories after a brief introductory section on general network terminology and notation. 

### Basic Network Terminology and Notation {#netterm}

Formally, a network is a collection of nodes and the set of ties between them. Nodes are also referred to as vertices primarily in the graph theory literature or actors in the sociology literature, while ties are also called edges or relationships. In graph theory, a network is defined with respect to its nodes and edges, and a network $G$ is equivalently written as $G(\mathcal{N}, \mathcal{E})$, where $\mathcal{N}$ is the collection of nodes, or nodeset, and $\mathcal{E}$ is the collection of edges, or edgeset. Typically, the nodes are numbered so that $\mathcal{N} = {1, 2, \dots, n}$, where $n$ is the total number of nodes in the network. The edgeset $\mathcal{E}$ is usually described as a set of pairs, written as $(i,j)$ or $i \mapsto j$ or simply $ij$, where $i \neq j \in \mathcal{N}$. In an undirected network, the ordering of $i$ and $j$ does not matter: there is no parent-child relationship, to use a term from graph theory, just a connection of some kind. In a directed graph, however, the order does matter: the tie $(i,j)$ is not equivalent to the tie $(j,i)$. In an undirected graph, the number of possible edges is $\binom{n}{2}$, while in directed graphs it is $n(n-1)$, assuming no self-loops (also called self-ties or simply loops) and only allowing for at most one edge between any two nodes. 

In statistical network analysis, a network is denoted by $x$ if it is observed or by $X$ if it is being treated as unobserved or as a random variable. Following this convention, edges in the networks $x$ or $X$ are denoted by $x_{ij}$ or $X_{ij}$, respectively. If the edge $i \mapsto j$ is present, $x_{ij} = 1$, whereas $x_{ij} = 0$ if the edge is not present. If $x$ is undirected, then $x_{ij} = x_{ji} \forall i \neq j \in \mathcal{N}$. If $x$ is directed, then $x_{ij}$ may equal $x_{ji}$, but this is not required and should not be assumed. Note that the definition of binary edge variables makes the assumption that edges are unweighted and that their cannot be more than one edge between two nodes. There are graphs and networks with weighted edges or with multiple ties between nodes, such as correlation networks used for modelling fMRI data or network-based epidemic modeling with finite, discrete state spaces (@Kolaczyk2009). The models I discuss here, including the stochastic actor-oriented models that are my primary focus, are all for unweighted networks, though some allow for extension to weighted networks. 

A network $x$ can also be expressed as an $n \times n$ matrix of 0s and 1s called the adjacency matrix, denoted $\mathcal{A}(x)$. The $ij^{th}$ entry of this matrix, $a_{ij}$ is 1 if there is an edge between nodes $i$ and $j$ and 0 otherwise. Typically, in statstical network analysis, the diagonal entries of this matrix, $a_{ii}$ are structurally 0, as self-ties or self-loops are not allowed or do not make sense. 

### Static Network Models {#staticnets}

The Erd&#x00F6;s-R&#x00E9;nyi random graph model is widely regarded as the first random graph model (@goldenberg09). In this model, first introduced in 1959 in  @er59, a random, undirected graph or network, $G$, is described in terms of its nodeset, $N$, and its edgeset, $E$, where $E$ is a random subset of the $\binom{|N|}{2}$ possible edges in the nodeset. The parameter in this model is $p$, the probability of an edge between any two nodes in $N$. The likelihood is written in terms of $|E|$ and $p$, 
$$f_G(|E| | p, N) = p^{|E|}(1-p)^{\binom{|N|}{2} - |E|}.$$
The properties and asymptotic behavior of this network model are well-established (@goldenberg09). Nodes in networks generated using this model will all have about the same degree, or number of incident edges, which is a very unrealistic property for networks to have. As such, many other models have been devised over the years as a way to better capture the the network creation process underlying real-world networks. 

A set of models which has also been very extensively studied is the exponential random graph family of models (ERGMs). The first, less general form of these models is the $p_1$ model for social networks, first introduced in @p1model. The $p_1$ model was developed for modelling directed networks, also called directed graphs or digraphs. In this model, the edges state, $(x_{ij}, x_{ji})$ between a pair of nodes, $(i, j)$ for all $i \neq j \in N$, could exist in one of four possible states: $(0,0)$ (no ties between $i,j$), $(1,0)$ (a tie from $i$ to $j$), $(0,1)$ (a tie from $j$ to $i$), or $(1,1)$ (a tie from $i$ to $j$ and from $j$ to $i$). Each one of these states has some probability such that the four state probabilities sum to 1 for each pair $(i,j)$. These probabilities are described in terms of five kinds of parameters: $\theta$, a base rate for edge creation; $\alpha_i$, an effect for an outgoing edge with parent node $i$; $\beta_j$, an effect for an incoming edge with child node $j$; $\rho_{ij}$, an effect of  reciprocated ties; and $\lambda_{ij}$, a normalizing constant to ensure that the four possible edge states of have probabilities summing to 1. Below, let $x_{ij} = 1$ when the edge from $i$ to $j$ exists and $x_{ij} = 0$ otherwise, and let $x_{ji} = 1$ when the edge from $j$ to $i$ exists and $x_{ji} = 0$ otherwise. Then, the edge state, $(x_{ij},x_{ji})$ between nodes $i$ andd $j$ is modeled as:
$$\log f_X((x_{ij},x_{ji})| \lambda_{ij}, \alpha_i, \alpha_j, \beta_i, \beta_j, 
\theta, \rho_{ij}) = \lambda_{ij} + x_{ij}(\alpha_i + \beta_j + \theta) + x_{ji}(\alpha_j + \beta_i + \theta) + x_{ij}x_{ji}\rho_{ij} $$
If each reciprocation parameter, $\rho_{ij}$, were unique to each edge, there would be a lack of identifiability in the model, which can be remedied by (i) not having a reciprocation effect ($\rho_{ij} = 0$ for all $i \neq j$), (ii) having a constant effect for reciprocation ($\rho_{ij} = \rho$ for all $i \neq j$), or (iii) having edge-dependent reciprocation ($\rho_{ij} = \rho + \rho_i + \rho_j$). Assuming scenario (ii), the log-likelihood function for a network $X$ can be written in exponential family form:
$$\log f_X(x | \boldsymbol{\lambda}, \boldsymbol{\alpha}, \boldsymbol{\beta}, \rho, \theta) \propto \theta \sum_{i,j} x_{ij} + \sum_i x_{i+}\alpha_i + \sum_j x_{+j}\beta_j + \rho\sum_{i,j}x_{ij}x_{ji},$$
where the minimally sufficient statistics are $x_{i+}$, the outdegree of each node $i$, $\sum_j x_{+j}$, the indegree of each node $j$, and $\sum_{i,j}x_{ij}x_{ji}$, the number of reciprocal ties in the network. This $p_1$ set of models is problematic because, as "the number of {$\alpha_i$} amd {$\beta_j$} increase directly with the number of nodes, [so] we have no consistency results for the maximum likelihood estimation" (@goldenberg09, p. 28). An extension of the $p_1$ model is the $p_2$ model, introduced by @p2model. This model is essentially a mixed-effects model version of the $p_1$ model, with node-level fixed effects for the outgoing edge effects $\boldsymbol{\alpha}$ and the incoming edge effects $\boldsymbol{\beta}$, and edge-level fixed effects for the edge rate effects $\boldsymbol{\theta}$ and reciprocity rates $\boldsymbol{\rho}$. The random effects, added to the covariate effects for $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$, have normal distribution with mean zero and variance parameters $\sigma^2_{\alpha}$ and $\sigma^2_{\beta}$. 

The final form for ERGMs is far more general than the $p_1$ and $p_2$, and is for undirected, rather than directed, networks. There are many more possible sufficient statistics other than the outdegree, indegree, and reciprocal ties. Other graph structures that are considered are the number of triangles, $T(X) = \sum_{i \neq j \neq h} x_{ij}x_{ih}x_{jh}$, and the number of $k$-stars, $S_k(X) = \sum_i \binom{x_{i+}}{k}$, where $k = 2$ is most commonly chosen. The form of the likelihood for $X$ following this general type of ERGM is 
$$f(X|\boldsymbol{\theta}, \tau) = \exp\left(\sum_k \theta_k S_k(X) + \tau T(X) + \psi(\theta, \tau)\right),$$ 
where $\theta_k$ and $\tau$ are parameters and $\psi(\theta, \tau)$ is the normalizing constant.  A problem with this model arises when one considers the nested nature of the sufficient statistics. For example, an edge can be contained in a 2-star, which can be contained in a triangle. So, the sufficent statistics can be dependent. Despite this flaw, this type of ERGM has been studied extensively, and many methods for parameter estimation exist, for example in the \texttt{R} packages \texttt{statnet} and \texttt{sna} (, @Rsoft,@statnet, and @sna). 

Another set of models includes extensions of the Erd&#x00F6;s-R&#x00E9;nyi (ER) random graph model. A natural way to extend the ER model is to vary the expected node degree of the graph. These models include the preferential attachment model or the small world models, which will be discussed further in \@ref(dynamicnets). Another model type, the exchangeable random graph model of @exchange, adds weak dependence into the edge sampling procedure.

Another area of study is community detection or blockmodels. The goal of these models is to simplify the network into a small number of "hubs" in which the members of the same hub form ties much more often with each other than with members of different hubs. These "hubs" are usually referred to as blocks or communities. Two nodes are in the same block if they are found to be "structurally equivalent," meaning that they have similar connectivity with other nodes in the network (@goldenberg09, p. 34). The model is defined as follows: given $n$ nodes and $K$ blocks, two nodes $i,j \in \mathcal{N}$ belong to the same block $h$, $h \in \{1, \dots, K\}$ if their neighborhoods $\mathcal{C}_i \equiv \{k \in \mathcal{N} : x_{ik} = 1\}$ and $\mathcal{C}_j \equiv \{k \in \mathcal{N} : x_{kk} = 1\}$ are approximately equal. Approximately equal is vague because the metric used to compute the difference between two neighborhoods $\mathcal{C}_i, \mathcal{C}_j$ can vary depending on context. This is similar to clustering but is "a more general task than clustering" (@goldenberg09). 

The last type static network model I'll present here is the latent space model. These models are called latent space models because they assumes that all nodes in the network can be expressed as a point in some $k$-dimensional space for "small" values of $k$ (@latentspace). These models condition on the unknown locations of the nodes, $\mathbf{Z}_i \in \mathbb{R}^k$. Thus, the conditional probability model for the adjacency matrix $Y$, where $y_{ij} = 1$ for an edge between $i,j$ and $y_{ij} = 0$ otherwise, can be written in the form 
$$Pr(Y|\mathbf{Z}, \mathbf{X}, \boldsymbol{\Theta}) = \prod_{i\neq j} Pr(y_{ij}|\mathbf{Z}_i, \mathbf{Z}_j, \mathbf{X}_{ij}, \boldsymbol{\Theta}),$$
where $\mathbf{X}$ are covariates, $\boldsymbol{\Theta}$ are parameters, and $\mathbf{Z}$ is the matrix of node positions in $\mathbb{R}^k$. The individual edge probabilities $y_{ij}$ are modeled in terms of  $\boldsymbol{\Theta} \equiv (\alpha, \boldsymbol{\beta})$:
$$\text{logit}(Pr(y_{ij} = 1)) = \alpha + \boldsymbol{\beta}'\mathbf{X}_{ij} - |\mathbf{Z_i} - \mathbf{Z_i}| \equiv \eta_{ij}.$$
This leads to the log likelihood function, 
$$\log(Pr(Y | \boldsymbol{\eta}) = \sum_{i \neq j} (\eta_{ij}Y_{ij} - \log(1+e^{\eta_{ij}}).$$
This model can also be extended to include edge weights. 

### Dynamic Network Models {#dynamicnets}

Dynamic network models are "the neglected sibling of static network" models (@goldenberg09, p. 41). Dynamic networks, however, are extremely important because of how realistic they are. Social networks do not form spontaneously: they evolve over time. Ties can be added and deleted, and new nodes can join the network. Modeling the process of network changes over time is more complex but ultimately more useful if done correctly. Like with static models, the literature on dynamic network models begins with fairly straightforward random graph models that are extensions of the classic Erdös-Rényi model. 

#### Quasi-Dynamic Models {#quasidynamo}

A model is quasi-dynamic if it models a static network via an underlying dynamic process. The first is the quasi-dynamic preferential attachment model of @pamodel. Given $n_0$ nodes to start, at each time point $t$ a new node is added with $n_t \leq n_0$ ties to the nodels already in the network. The $n_t$ new ties are assigned proportionally based on the degree of each existing node. It is quasi-dynamic because it is usually used to model one scale-free network oberervation. The preferential attachment model is also referred to as the "rich-get-richer" model because it results in a network where there are a few nodes with very high degree. 

Another quasi-dynamic model is the small-world model of @Watts98. Given $n$ nodes to start, each with $k$ edges that form a ring lattice (nodes layed out in a circle and connected to their $k$ closest neigbors), edges are randomly "rewired" with probability $p$. This results in networks with the small-world property: let $L$ be the average distance between any two nodes in the graph, and if the graph has the small-world property, $L \propto \log(n)$ as $n$ increases (@Watts98).

The last quasi-dynamic model is the duplication-attachment model originally studied in computer science theory in order to study the structure of the world wide web (@goldenberg09). This model is for directed, rather than undirected graphs like the previous two models. Generally speaking, this model is constructed as follows: start with a graph $G$ with nodeset $\mathcal{N}$ and edgeset $\mathcal{E}$. At each time point $t$, one new node is added to $G$. This node is connected to a "prototype" node, call it $m$, that was selected at random from $\mathcal{N}$. In addition to this connection to node $m$ (from $m$ to the new node), there are $d$ links added from the new node to other nodes in $\mathcal{N}$. Each new link is added randomly with probability $\alpha$ to one of the nodes in $\mathcal{N}$, selected with uniform probability. With probability $1-\alpha$, the new link is directed to a node which is linked from $m$, say $\ell$ so that a path from $m$ to $\ell$ through the new node. This model can be expressed in many different ways, but I do not present them here as they are not related to the dynamic model I have chosen to study. 

#### Truly Dynamic Models {#truedynamo}

XXX Discuss discrete-time models (ERGM, latent space, DCFM) XXX

XXX Discuss continuous time markov models (lead in to next section) XXX

### Placing Network Analysis in the Statistical Framework

Network analysis has earned much interest in fields suchs as sociology or computer science. Many methods of analysis for networks have been derived from researchers in these fields, and the interest in network analysis in these fields has grown exponentially throughout the last two decades. There are now entire journals dedicated to network analysis, including *Network Science* and *Social Networks*. In the field of statistics, however, not much attention has been given to this area. There are several possible reasons for this, the first being that statistical network analysis does not fit easily into the statistician’s workflow and way of thinking. 

First and foremost, when modelling, statisticians consider the population from which their data were generated. In some social network problems, this is easy, like in the students at a university example. In others, however, it might not be so straightforward, like when mapping the interconnectedness of scientific disciplines (@Kolaczyk09). Next, statisticians consider the representativeness of their data. Usually, a random sample of the population has been carefully selected for analysis to be representative. But in network analysis, there may not be a well-designed, appropriate sampling procedure. Social media networks have become very popular and very large, so many researchers have interest in studyin them, but how can they be sampled? Additionally, how does one avoid biases in social network sampling, and can these biases be corrected for (@Kolacyk09). If one wants to summarize their network data, what statistics should be used? Measure such as like mean and variance are available and presented for most types of statistical, but there is no definition for a "mean" or a "variance" on a network. So, other statistics like average outdegree are often used. But ultimately, these statistics cannot describe the network structure as well as the mean and variance can describe the distribution of a random variable. As was shown in section \@ref(models) there are many models for network analysis, but once a network is modeled, how can we make predictions for what new networks will be or what new edges will form? 

Furthermore, traditional statistical models have some common properties that network models might not always have. First, the models have to be well-grounded in measure and probability theory so that their behavior can be well-understood based on the fundamentals of these areas of study. This may be true in many network models, but they may also be analytically intractable. Second, the models have to be estimable from the data at hand. Again, this is possible in many of the network models, but some may only be estimable with advanced simulation methods that have only recently become available with respect to computing power. Next, a crucial element within statistics is the quantification of uncertainty in estimation. Since there is not a measure of "variance" on a network, how then can the estimation of network model parameters be qualified to include uncertainty? 

Now that statisticians are increasingly becoming interested in network analysis, many of the “holes” have been filled. The theory and methodology underlying statistical network analysis, however, show some gaps in a few important areas.  WHAT ARE THE AREAS?


## Stochastic Actor-Oriented Models for Longitudinal Social Networks. {#saoms}

The phrase Stochastic Actor-Oriented Model is quite a mouthful, but it contains most of the important information about the model. First, the model is changing in time in order to accomodate for observations from the same network made at different points in time. Second, it allows for changes in network structure due to actor-level covariates. These two properties are crucial to understanding networks as they exist naturally. Most social networks are ever-changing as relationships decay or grow, and most actors (or nodes) in social networks have inherent properties that could affect how they change their place within the network. 

### Terminology, Notation, and Mathematical Definition of SAOMs {#saomsnote}

A longitudinal network is a network consisting of the same set of $n$ nodes that is changing over time, and is observed at $M$ discrete time points, $t_1, \dots, t_M$. Denote these network observations $x(t_1), \dots, x(t_M)$. The SAOM assumes that this longitudinal network is embedded within a continuous time markov process (CTMP), call it $X(T)$. This process is almost entirely unobserved. The process $X(T)$ theoretically exists outside of the range of observation, but for simplicity of notation, assume that the beginning of the process, $X(0)$ is equivalent to the first observation $x(t_1)$, while the end of the process $X(\infty)$ is equivalent to the last observation $x(t_M)$.  The observations $x(t_1), \dots, x(t_M)$ are observed states of the process, $x(t_1) \equiv X(0), x(t_2) \equiv X(T_{t_2}), \dots, x(t_{M-1}) \equiv X(T_{t_{M-1}}), x(t_M) \equiv X(\infty)$, but the time points $t_m$ and $T_{t_m}$ for $m = 2, \dots M-1$ are not equivalent. The process $X(T)$ is a series of single tie changes, in which one actor at a time is given the opportunity to add or remove one outgoing tie.  These opportunities for change can arise at a different rate for each actor, and the overall rate of change, the distribution of the waiting times that *any* actor will be given the opportunity to change is a function of all actors' rates. Additionally, once an actor is given the chance to change a tie, it tries to maximize a sort of utility function based on the current and potential future states of the network. These functions are described in detail in subsections \@ref(saomrate) and \@ref(saomobjective). 

### The Rate Function {#saomrate}

In the network $x$ and for each actor $i$ in this network, the rate function is most generally denoted $\lambda_i(\alpha, \rho, x, m)$, which dictates how quickly actor $i$ gets opportunities to change one of its ties, $x_{ij}$ in the time period $t_{m} \leq T < t_{m+1}$. In this function, $\alpha$ and $\rho$ are parameters, $x$ is the current network state at time $m$. Note that $x \in \mathcal{X}$, where $\mathcal{X}$ is the space of possible networks given the $n$ nodes in the network, and that $|\mathcal{X}| = 2^{n(n-1)}$. We assume that the actors $i$ are conditionally independent given their current ties, $x_{i1}, \dots, x_{in}$. This assumption gives the rate function for the whole network as $\lambda(\alpha, \rho, x, m) = \sum_i \lambda_i(\alpha, \rho, x, m)$. For any time point, $T$, where $t_m \leq T < t_{m+1}$, the waiting time to the next change opportunity by actor $i$ has distribution *Exponential*$(\lambda_i\alpha, \rho, x, m))$ in order to achieve the memorylessness property of a Markov process. Thus, the waiting time to the next change opportunity by *any* actor in the network has distribution *Exponential*($\sum_i \lambda_i\alpha, \rho, x, m)$). There are many possibilities for the rate function, $\lambda_i$. The simplest is that it is constant over all actors and all unobserved timepoints between observations $x(t_{m-1})$ and $x(t_m)$, $\lambda_i(\alpha, \rho, x, m)) = \alpha_m$. The rate function can also depend on covariate values, call them $\mathbf{z}_{i}(t_m)$, of the actors or structural network elements such as outdegree, or both. For instance, assume $\lambda_i(\alpha, \rho, x, m)) = \lambda_{i1}\lambda_{i2}\lambda_{i3}$, where $\lambda_{i1}$ is constant over $m$, $\lambda_{i2}$ depends on the actor covariates, and $\lambda_{i3}$ depends on a structural network property for node $i$. $\lambda_{i2}$ might be written as $\lambda_{i2} = \exp\left(\sum_h \rho_h z_{ih}(t_m)\right)$, where there are $h = 1, \dots, H$ actor covariates of interest, each with their own parameter $\rho_h$. $\lambda_{i3}$ can be written as a function of the outdegree of node $i$, denoted $x_{i+}$ with its own parameter $\alpha_{H+1}$, so that, for example, $\lambda_{i3} = \frac{x_{i+}}{n-1}\exp(\alpha_{H+1}) + \left(1-\frac{x_{i+}}{n-1}\right)\exp(-\alpha_{H+1})$. When $H=0$, this form of $\lambda_{i3}$ is equivalent to the model proposed by @wassermanrecip, which is one of the first models proposed for modeling dynamic networks as continuous-time Markov processes (@snijders01).  Given that a change occurs, the probability that actor $i$ is given the power to change a tie is $$\frac{\lambda_i(\alpha, \rho, x, m))}{\sum_i \lambda_i(\alpha, \rho, x, m))}$$.
 
### The Objective Function {#saomobjective} 

Thanks to the conditional dependence assumptions in the model, we can consider the objective function for each node separately, since only one tie from one node is changing at a time. The objective function is written as $f_i(\boldsymbol{\beta}, x) = \sum_k \beta_k s_{ik}(x, \mathbf{Z}), x \in \mathcal{X}$ and $\mathbf{Z}$ the matrix of covariates. The vector $\boldsymbol{\beta}$ are the parameters of the model and $x$ is any possible state of the network. Given the focal or ego node, $i$, there are $n$ possible steps for the actor $i$ to take: either one of all current ties $x_{ij}$ will be destroyed, a new tie will be created, or no change will occur. 

The parameters, $\beta$, are attached to various actor-level network statistics, $s_{ik}(x)$. There are always at least two parameters, $\beta_1$ for the outdegree of a node, and $\beta_2$ for the number of reciprocal ties held by a node (@snijders01 p. 371). There are many possible parameters $\beta$ to add to the model. They can be split up into two groups: first, the structural effects, which only depend on the structure of the network. The inclusion of these effects has origin in the ERGMs discussed previously for static networks in section \@ref(staticnets). These effects are written in terms of the edge variables $x_{ij}$, for $i \neq j$. The second set of effects are the actor-level or covariate effects. These effects also depend on the structure of the network. They are written in terms of $x_{ij}$ but also in terms of the covariates, $\mathbf{Z}$. A table of some possible structural and covariate effects is given in \@ref(tab:saom-effects). 

\begin{table}
\caption{(\#tab:effects) Some of the possible effects to be included in the stochastic actor-oriented models in RSiena. There are many more possible effects, but we only consider a select few here. For a complete list, see the RSiena manual (@RSiena).}
\centering
\begin{tabular}{ll}
\textbf{Structural Effects} \\
outdegree  & $s_{i1}(x) = \sum_j x_{ij}$ \\
reciprocity  & $s_{i2}(x) = \sum_j x_{ij}x_{ji}$ \\
transitive triplets  & $s_{i3}(x) = \sum_{j,h} x_{ij}x_{jh}x_{ih}$ \\
\textbf{Covariate Effects}\\
covariate-alter  & $s_{i4}(x) = \sum_j x_{ij}z_j$ \\
covariate-ego  & $s_{i5}(x) = z_i\sum_j x_{ij}$ \\
same covariate & $s_{i6}(x) = \sum_j x_{ij} \mathbb{I}(z_i = z_j)$ \\
jumping transitive triplets  & $s_{i7}(x) = \sum_{j \neq h} x_{ij}x_{ih}x_{hj} \mathbb{I}(z_i = z_h \neq z_j)$
\end{tabular}
\end{table}

When node $i$ is given the chance to change a node, we assume that they wish to maximize the value of their objective function $f_i(\boldsymbol{\beta}, x)$ plus a random element, $U_i(x)$, where the $U_i(x)$ are from "the type 1 extreme value distribution (or Gumbel distribution) with mean 0 and scale parameter 1" (@snijders01, p. 368). This distribution, which is also known as the log-Weibull distribution, has probability distribution function, using $\mu$ for the mean parameter and $\sigma$ for the scale parameter, of

$$f(u|\mu, \sigma) = \frac{1}{\sigma}\exp\left\{-\left(\frac{x-\mu}{\sigma} + e^{-\frac{x-\mu}{\sigma}}\right)\right\}.$$

Using this distribution is convenient because it allows the probablity the actor $i$ chooses to change its tie to actor $j$ in terms of the objective function alone. Let $p_{ij}(\boldsymbol{\beta}, x)$ be this probability. Next, write the network $x$ in its potential future state, where the tie $x_{ij}$ has changed to $1-x_{ij}$, as $x(i \leadsto j)$. Then, the probility that the tie $x_{ij}$ changes is 

$$p_{ij}(\boldsymbol{\beta}, x) = \dfrac{\exp\left\{f_i(\boldsymbol{\beta}, x(i\leadsto j))\right\}}{\sum_{h \neq i} \exp\left\{f_i(\boldsymbol{\beta}, x(i \leadsto h))\right\}}$$

### A SAOM as a CTMP

In order to fit this model definition back into the original context of the CTMP described in section \@ref(truedynamo), it must be written in terms of its intensity matrix, $\mathbf{Q}$.  This matrix describes the rate of change between states of the process. For networks, there are a very large number of possible states, $2^{n(n-1)}$, so the intensity matrix is a square matrix of that dimension. But, thanks to the property of SAOM that the states are allowed to change only one tie at a time, there are only $n$ possible states given the current state, $n-1$ of which are uniquely determined by the node $i$ that is given the opportunity to change. Thus, the intensity matrix $\mathbf{Q}$ is very sparse, with only $n(n-1) + 1$ non-zero entries in each row. Note that $n(n-1)$ of these represent the possible states that are one edge different from a given state, and the additional non-zero entry is for the state to remain the same. All other entries in a row are zero because those column states cannot be reached from the row state by just one change as dictated by the SAOM. The entries of $\mathbf{Q}$ are defined as follows: let $b \neq c \in \{1, 2, \dots, 2^{n(n-1)} \}$ be indices of two different possible states of the network, $x^b, x^c \in \mathcal{X}$.  Then the $bc^{th}$ entry of $Q$ is:
\[ q(x^b, x^c) = \begin{cases} 
      \lambda_i(\alpha, \rho, x^b, m)p_{ij}(\boldsymbol{\beta}, x^b) & \text{if } x^c \in \{x^b(i \leadsto j) | \text{ any } i \neq j \in \mathcal{N}\} \\
      0 & \text{if } x^c \text{ differs from } x^b \text{ by more than 1 tie} \in \mathcal{N} \\
      -\sum_{i\neq j} \lambda_i(\alpha, \rho, x^b, m)p_ij(\boldsymbol{\beta}, x^y) & \text{if } x^b = x^c 
   \end{cases}
\]

Thus, the rate of change between any two states that differ by only one tie, $x_{ij}$, is the product of the rate at which actor $i$ gets to change a tie and the probability that the tie that will change is the tie to node $j$.^[Just to be clear, the change is from $x^b_{ij}$ to $x^c_{ij} = 1 - x^b_{ij}$.] Furthermore, the theory of continuous time Markov chains gives that the matrix of transition probabilities between observation times $t_{m-1}$ and $t_{m}$ is dependent only on the difference between timepoints, $t_m - t_{m-1}$ and is equal to $e^{(t_m - t_{m-1})\mathbf{Q}}$, where $\mathbf{Q}$ is the matrix defined above and $e^X$ for a real or complex square matrix $X$ is equal to $\sum_{k=0}^{\infty} \frac{1}{k!} X^k$.


## Model Fitting and Diagnostics for SAOMs



## What is Visual Inference?

The concept of visual inference was first introduce in @Bujaetal. In this seminal work, the authors outline two protocol for visual tests of hypotheses, the "Rorschach" the "lineup". The former allows one "to measure a data analyst’s tendency to overinterpret plots in which there is no or only spurious structure," while the latter has the viewer "identify the plot of the real data from among a set of decoys [...] under the veil of ignorance" (@Bujaetal, p. 4368-9). 

Viewing plots of data is an important part of exploratory data analysis (EDA) and of model diagnostics (MD). In EDA, plots guide the analyst on their quest to choose a model, while in MD, plots help the analyst determine if the model chosen is appropriate. In EDA, the analyst may notice that a covariate is strongly correlated with the dependent variable by drawing a scatterplot, leading the analyst to choose a simple linear model. But in MD, the analyst could later notice a pattern in the residuals plotted against the covariate, indicating that the variance of the dependent variable is not constant across changing values of the covariate. These steps of EDA and MD have become so engrained in statistical practice that they are taught in introductory statistics courses. 


## Network Visualization

Network visualization, also called network mapping, is a very well-established subfield of network analysis. As networks have such a non-traditional data structure, visualization has always been of the utmost importance to understanding the structre of a network.

### Layout Algorithms

The key difficulty with network visualization that does not arise with most other types of data visualization is the lack of a well-defined axis. This is not something one has to think hard about for most data visualizations. If the variables are numerical, histograms, scatterplots, or time series plots are straightforward to construct: one variable on the x-axis, another on the y-axis in 2D Euclidean space. If the variables are categorical, bar charts and mosaic plots can be constructed in this same space. If the data are spatial, there is a well-defined space In pretty much any case, the location and labels of the data and axes can be defined with very little struggle. With network data, however, this is a more difficult problem. 

Network visualizations are made by representing nodes with points in 2D Euclidean space, just like one would with any other data set, and then by representing edges by connecting the points with lines if there is an edge between the two nodes. But, because there is no natural placement of the points, a random placement is used, then adjusted iteratively via a layout algorithm, of which there are many kinds. I will focus on the 2D layout algorithms only because I work later with the `ggplot2` package to visualize networks, and this package only has 2D drawing capabilities.

Some layout algorithms were designed to mimic physical systems, drawing the graphs based on the "forces" connecting them. The network's edges act as springs pushing and pulling the nodes in 2D space. The force-directed layout algorithms are:

- Kamada-Kawai
- Fruchterman-Reingold
- Spring embedding
- Target diagram
- Graphopt
- OpenORD or VxORD


Other layout algorithms depend on the mathematical properties of the network's adjacency matrix or some other function or propterty of the network. Algorithms of this kind are:

- Eigen
- Hall
- Multidimensional Scaling (MDS)

Some layout algorithms only exists for certain types of networks:

- Reingold-Tilford: for trees
- Sugiyama: for layerd directed acyclic graphs

Finally, some layout methods just place the nodes randomly or in a simple ordering:

- Random: according to some distribution
- Grid: 
- Circle: 

### R Packages

There is a multitude of R packages that exist for network analysis, and many, if not most, of them contain some sort of built-in functionality for visualizing networks. The most popular of these is probably the `igraph` package by @igraph. This package is extensive, and contains much more than methods for network visualization. It contains tools for both 2D and 3D visualization of networks. The 2D layouts it contains are `random`, `circle`, `star`, `grid`, `graphopt`, `bipartite`, `fruchterman_reingold`,`kamada_kawai`, `mds`, `grid_fruchterman_reingold`, `lgl`, `reingold_tilford`, `reingold_tilford_circular`, and `sugiyama`.  

Another popular package for network analysis is `sna` by @sna. This package was designed specifically for social network analysis (sna), so it also contains much more capabilities for network analysis in addition to visualization. Like `igraph`, `sna` contains both 2D and 3D layout methods. The 2D layout algorithms available in `sna` are `circle`, `circrand`, `eigen`, `fruchtermanreingold`, `geodist`, `hall`, `kamadakawai`, , `mds`, `princoord`, `random`, `rmds`, `segeo`, `seham`, `spring`, `springrepulse`, and `target`. 

Research into possible layout algorithms is important, but it ignores some of the things that statisticians usually consider when visualizing data. For instance, since the location of points in 2D space contains no information about the data, how else should this information be visualized? As an example, consider a friendship network of students at a university. Representing this network as simple points and lines leaves a lot of information out. Some information that could be incorporated includes the students’ majors, year in school, and whether the students have ties through their classes or their extracurricular activities. In the network visualization, this information can be mapped to color of point, shape of point, and linetype, respectively. Adding this aesthetic information helps to make up for the loss of two dimensions of visual perception and to bring the network visualization into the world of statistical graphics.   

### The Importance of `ggplot2`

## Lead-in to Thesis